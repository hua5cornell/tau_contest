\section{Parallelization}
\label{sec:parallel}

\subsection{The Operator Formulation}

We leverage parallelization to achieve fast design optimization. To do this, we analyze the parallelism available in our optimizer with the operator formulation, a {\em data-centric} abstraction that presents a {\em local view} and a {\em global view} of algorithms~\cite{pingali11}.

The local view is described by an {\em operator}, a graph update rule applied to {\em active nodes}, which are nodes in the graph where there is computation to be done. Each operator application, called an {\em action}, may read from/write to a set of nodes and edges around the active node, termed the {\em neighborhood} of the action. Active nodes become inactive once the actions are finished.

The global view is captured by (1) the location of active nodes, and (2) the order by which the actions must appear to be carried out. For {\em unordered} algorithms, e.g., chaotic relaxation for SSSP, processing active elements in any order gives the same answer. However, some orderings may be more efficient than the others.

%{\em Ordered} algorithms, on the other hand, require active elements to be processed as if by certain ordering in order to give correct results~\cite{hassaan11}. For instance, discrete event simulation needs to follow time ordering among events.

Algorithms can be categorized as {\em data-driven} or {\em topology-driven} based on the pattern of active nodes. A data-driven algorithm begins with a set of initially active nodes, generates new active nodes on the fly, and terminates when there are no more active nodes to be processed. Dijkstra's single-source shortest path (SSSP) algorithm is an example. In contrast, a topology-driven algorithm makes sweeps over all nodes until certain convergence criteria is reached. Bellman-Ford algorithm is an example. Although this discuss has focused on algorithms in which nodes are active, the same approach works for algorithms in which \emph{edges} are active so in general, we refer to active \emph{elements} in the graph.

Parallelism in graph algorithms can be exploited by parallel executiion of actions with disjoint neighborhoods.

\subsection{Available Parallelism}
\label{sec:avail_parallelism}

With the operator formulation of algorithms, we are ready to analyze the parallelism available in our design optimizer, composed of static timing analysis, buffer insertion, and gate sizing using slew targeting.

\subsubsection{Static Timing Analysis}
\label{sec:sta_parallel}

Given a synchronous design, STA represents the design as a timing graph $G = (V, E)$, a DAG, where nodes in $V$ are the pins and $(u, v) \in E$ are wires or timing arcs between pins. For an edge $(u, v)$, we say $u$ is $v$'s predecessor and $v$ is $u$'s successor. STA then computes for all pins their arrival times and slew rates in topological order from primary inputs (forward propagation); and computes required times and slacks in reverse topological order from primary outputs and constrained pins, e.g., register inputs (backward propagation).

As the timing graph for a synchronous design is a DAG, the processing order in STA can be enforced by explicitly tracking the number of unresolved dependencies for each pin. A pin can be processed if all its dependencies are resolved, and all the pins whose dependencies are cleared at the same time can be processed in parallel.

Specifically, for each pin $v$, let $dep(v)$ be its number of unresolved dependencies, $pred(v)$ its predecessors, and $succ(v)$ its successors. For forward propagation, initially $dep(v) = |pred(v)|$; after pin $u$ computes its arrival time and slew rate, $u$ atomically decrements $dep(v)$, $v \in succ(u)$. For backward propagation, initially $dep(v) = |succ(v)|$; after a pin $w$ computes its required time and slack, $w$ atomically decrements $dep(v)$, $v \in pred(w)$. For both forward and backward propagations, pin $v$ becomes active when $dep(v) = 0$, and pins whose $dep$s are zero at the same time can be processed in parallel.

As STA needs to track active pins explicitly, STA is a {\em data-driven} algorithm. All active pins can be processed in any order, so STA is an {\em unordered} algorithm. Note that the ordering in STA only defines when pins should become active but not the processing order of active pins.

\subsubsection{Buffer Insertion}

\paragraph{For fixing maximum load constraints:} As explained in Section~\ref{sec:buf_insert}, a buffer is inserted in between a gate output pin, $v$, and its load, $load(v)$, if $load(v) > maxC(v)$, where $maxC(v)$ is the maximum load that can be driven by $v$.
Every gate output can insert such a buffer independently, so this is a {\em topology-driven}, {\em unordered} algorithm.

\paragraph{For fixing hold time violations:} Recall from Section~\ref{sec:buf_insert} that we insert buffers in a design to fix hold time violations round by round. In each round, we run STA to identify the most-critical hold-time path, and insert buffers to the wire segment having the largest setup-time slack on the path. Since there is only one active edge in a round, our buffer insertion scheme contains no parallelism. It is a {\em data-driven} algorithm, since the active edge in a round depends on the circuit timing in the round.

\subsubsection{Gate Sizing with Slew Targeting}

Recall from Section~\ref{sec:gate_sizing} that gate sizing with slew targeting~\cite{Held:Gate} is a round-based algorithm. In each round, a full STA is performed first, then all pins set their slew targets, and then all gates select their cells. Finally, the new cell assignment is scored and then kept or reverted.

\paragraph{Setting slew targets:} Slew targets can be set for each pin independently in any order, since each pin $p$ can get from STA the required information for computing new slew target in the next round: current slew, $p$'s own slack, and the neighboring gates' pins' slacks. Therefore, setting slew targets is a {\em topology-driven}, {\em unordered} algorithm.

\paragraph{Cell assignment:} The parallelism available in assigning cells to gates is similar to that in computing required times and slacks in STA. As mentioned in~\ref{sec:sizing_order}, gates should be sized in reverse topological order on the graph of gate connectivity, with edges feeding into register data inputs ignored. Instead of constructing a gate connectivity graph and then ignore some edges in it, we can enforce the order of sizing gates with the idea that a gate should be processed after all its pins are processed. This reduces the memory space requirement of ParallelClosure by utilizing the timing graph for STA for gate sizing as well.

Specifically, we associate for each gate $g$ a counter, $untouched(g)$, to track the number of untouched pins for $g$. Initially $untouched(g) = |pin(g)|$, where $pin(g)$ is the set of pins belonging to gate $g$. The dependency among pins are tracked as in Section~\ref{sec:sta_parallel} for computing required times and slacks in STA. When a pin $v$ is processed, $v$ also atomically decrements $untouch(gate(v))$, where $gate(v)$ is the gate to which pin $v$ belongs. Gate $g$ becomes active when $untouched(g) = 0$. Initially, no gates are active, and pins with no successors are active. All gates with $untouched = 0$ simultaneously can be assigned to cells in parallel.

Similar to STA, cell assignment is a {\em data-driven}, {\em unordered} algorithm. Nevertheless, its operator is different from the one for computing required times and slacks in STA.

\paragraph{Scoring a cell assignment:} As mentioned in Section~\ref{sec:sizing_converge}, a new cell assignment is scored as a linear combination of worst negative slack, average total negative slack, and average cell area. The score can be computed by dividing the gates to threads to compute thread-local results, and then reduce all the thread-local results to the final answer. All gates and constrained pins can be processed in parallel. Therefore, scoring a cell assignment is a {\em topology-driven}, {\em unordered} algorithm.

\paragraph{Keeping/reverting a cell assignment:} Each gate can be processed independently, so this is a {\em topology-driven}, {\em unordered} algorithm.

\subsection{Implementation in Galois}

We implement our design optimizer using the Galois framework~\cite{nguyen:2013,Lenharth:2016}, a C++ library for parallel programming based on the operator formulation. The Galois framework (1) provides parallel data structures, and language constructs for highlighting parallelization opportunities; and (2) supports dynamic work generation, load balance, resource management, and transactional execution of operators.

All sub-algorithms in our design optimizer are implemented as described in Section~\ref{sec:avail_parallelism} except for buffer insertion, where everything is done sequentially to maintain the mapping consistency from names to gates/wires. The timing graph is constructed as follows: gates track their pins; pins track their wires and belonging gates, if any; and wires track their member pins. Cell assignment is associated with gates and pins consistently. All timing corners and delay modes are analyzed simultaneously for all algorithms. This is done by storing MCMM data for all pins, and processing all MCMM data when processing a pin/gate.
