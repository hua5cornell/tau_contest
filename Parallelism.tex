\section{Parallelization}
\subsection{The Operator Formulation}

We leverage parallelization to achieve fast design optimization. In order to do so effectively, we analyze the parallelism available in our optimizer with the operator formulation~\cite{pingali11}, a {\em data-centric} abstraction of algorithms.

The operator formulation starts from identifying the data structures involved in the algorithm, e.g., a graph. {\em Active elements} capture where in the graph the computation needs to be done. An {\em operator} specifies the rules to update the graph, and it will be applied to active elements. Each application of an operator to an active element is called an {\em action}. An action may need to read from or write to a set of nodes and edges around the active element, which is termed the {\em neighborhood} of the action. Active elements become inactive once the actions are finished.

Algorithms can be categorized as {\em data-driven} or {\em topology-driven} based on the pattern of active elements. A data-driven algorithm begins with a set of initially active elements, generates new active elements on the fly, and terminates when there are no more active elements to be processed. Dijkstra's single-source shortest path (SSSP) algorithm is an example. In contrast, a topology-driven algorithm makes sweeps over all nodes/edges until certain convergence criteria is reached. Bellman-Ford algorithm is an example.

Scheduling needs to be considered when there are multiple active elements at the same time. For {\em unordered} algorithms, e.g., chaotic relaxation for SSSP, processing active elements in any order gives the same answer. However, some ordering may be more efficient than the others.

%{\em Ordered} algorithms, on the other hand, require active elements to be processed as if by certain ordering in order to give correct results~\cite{hassaan11}. For instance, discrete event simulation needs to follow time ordering among events.

Parallelism in graph algorithms can be exploited among actions with disjoint neighborhoods and read-only operators.

\subsection{Available Parallelism}
\label{sec:avail_parallelism}

With the operator formulation of algorithms, we are ready to analyze the parallelism available in our design optimizer, composed of static timing analysis, buffer insertion, and gate sizing using slew targeting.

\subsubsection{Static Timing Analysis}
\label{sec:sta_parallel}

Given a synchronous design, static timing analysis (STA) represents the design as a timing graph $G = (V, E)$, a directed acyclic graph (DAG), where nodes in $V$ are the pins and $(u, v) \in E$ are wires or timing arcs between pins. For an edge $(u, v)$, we say $u$ is $v$'s predecessor and $v$ is $u$'s successor. STA then computes for all pins their arrival times and slew rates in topological order from primary inputs; and computes required times and slacks in reverse topological order from primary outputs and constrained pins, e.g., register inputs.

As the timing graph for a synchronous design is a DAG, the processing order in STA can be enforced by explicitly tracking the number of unresolved dependencies for each node. A node can be processed if all its dependencies are resolved, and all the nodes whose dependencies are cleared at the same time can be processed in parallel.

Specifically, each pin $v$ is associated with a counter, $dep(v)$, to track the number of unresolved dependencies for $v$ at a moment. For computing arrival times and slew rates, initially $dep(v) = |pred(v)|$, where $pred(v)$ is the set of $v$'s predecessors. When a pin $u$ finishes computing its arrival time and slew rate, $u$ atomically decrements $dep(v)$ for all $v \in succ(u)$, where $succ(u)$ is the set of $u$'s successors. Pin $v$ becomes active when $dep(v) = 0$. Initially only primary inputs are active. Pins can be processed in parallel if their $dep$s are zero at the same time.

For computing required times and slacks, initially $dep(v) = |succ(v)|$. When a pin $w$ finishes computing its required time and slack, $w$ atomically decrements $dep(v)$ for all $v \in pred(w)$. Pin $v$ becomes active when $dep(v) = 0$. Initially only pins with no successors, e.g., primary outputs, are active. Pins whose $dep$s are zero at the same time can be processed in parallel.

As STA needs to track active nodes explicitly, STA is a {\em data-driven} algorithm. All active nodes can be processed in any order, so STA is an {\em unordered} algorithm. Note that the ordering in STA only defines when nodes should become active but not the processing order of active nodes.

\subsubsection{Buffer Insertion}

Recall from Section ??? that we insert buffers to a design round by round. In each round, we run STA to identify the most-critical hold-time path, and insert buffers to the edge having the largest setup-time slack on the path. Since there is only one active edge in a round, our buffer insertion scheme contains no parallelism. It is a {\em data-driven} algorithm, since the active edge in a round depends on the circuit timing in the round.

\subsubsection{Gate Sizing with Slew Targeting}

Recall from Section ??? that gate sizing with slew targeting is a round-based algorithm. In each round, we run STA first, and then set slew targets for all pins, assign cells to all gates, evaluate improvement, and finally keep or revert the cell assignments for all gates.

\paragraph{Setting slew targets} Slew targets can be set for each pin independently in any order, since each pin only needs its slack and current slew to decide its slew target in the next round. Therefore, setting slew targets is a {\em topology-driven}, {\em unordered} algorithm.

\paragraph{Cell assignment} The parallelism available in assigning cells to gates is similar to that in computing required times and slacks in STA. As pointed out in~\ref{Held:Gate}, output driving capacitance impacts more on a gate's output slew and delay; hence, it is desirable to fix all cell assignments for the downstream gates of gate $g$ before assigning $g$ to a cell. Downstream gates of a gate $g$ are defined as the gates closer to register inputs or primary outputs than $g$ is, excluding registers.

The ordering of gates can be enforced with the idea that a gate should be processed after all its pins are processed. Associate for each gate $g$ a counter, $untouched(g)$, to track the number of untouched pins for $g$. Initially $untouched(g) = |pin(g)|$, where $pin(g)$ is the set of pins belonging to gate $g$. The dependency among pins are tracked as in Section~\ref{sec:sta_parallel} for computing required times and slacks in STA. When a pin $v$ is processed, $v$ also atomically decrements $untouch(gate(v))$, where $gate(v)$ is the gate $v$ belongs to. Gate $g$ becomes active when $untouched(g) = 0$. Initially, no gates are active, and pins with no successors are active. All gates with $untouched = 0$ simultaneously can be assigned to cells in parallel.

Similar to STA, cell assignment is a {\em data-driven}, {\em unordered} algorithm. Nevertheless, its operator is different from the one for computing required times and slacks in STA.

\paragraph{Improvement evaluation} A new cell assignment is scored as a linear combination of worst negative slack, average total negative slack, and average cell area. The lower the score, the better. The score can be computed by dividing the gates to threads to compute thread-local results, and then reduce all the thread-local results to the final answer. All gates and pins can be processed in parallel. Therefore, improvement evaluation is a {\em topology-driven}, {\em unordered} algorithm. 

\paragraph{Keeping/reverting cell assignment} Each gate can be processed independently, so this is a {\em topology-driven}, {\em unordered} algorithm.

\subsection{Implementation in Galois}

We implement our design optimizer using the Galois framework~\cite{nguyen:2013,Lenharth:2016}, a C++ library for parallel programming based on the operator formulation. The Galois framework (1) provides parallel data structures, and language constructs for highlighting parallelization opportunities; and (2) supports dynamic work generation, load balance, resource management, and transactional execution of operators.

All sub-algorithms in out design optimizer implemented as described in Section~\ref{sec:avail_parallelism}. The timing graph for a design is constructed using the {\tt MorphGraph} in the Galois framework. All timing corners and delay modes are analyzed simultaneously for all algorithms. This is done by storing MCMM data for all pins, and processing all MCMM data when processing a pin/gate.
